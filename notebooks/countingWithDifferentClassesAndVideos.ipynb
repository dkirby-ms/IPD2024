{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67aa4381",
   "metadata": {},
   "source": [
    "## Counting objects with YOLO8\n",
    "\n",
    "In this exercise, we will count a particular object in real-time using [YOLOv8](https://docs.ultralytics.com/models/yolov8/) object detection model. With this exercise, we will see how we can effectivily monitor not only static objects but also the objects as they move within a bounding box. We will also see how we can change the object that we want to count for different scenarios.\n",
    "\n",
    "This exercise is divided into steps. We start with the first step of installation and we end with the last step which is the excecution. The execution is the step will all us to tell the system which object we would like to use for object counting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84246e5",
   "metadata": {},
   "source": [
    "## Installation\n",
    "The first step is to run the following install steps.\n",
    "Click on the Play icon to the left of the cell below to install opencv-python, pyyaml and openvino."
   ]
  },
  {
   "cell_type": "raw",
   "id": "36548180",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "%pip install opencv-python pyyaml openvino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab46367",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "The next step is the import of the necessary libraries.\n",
    "Click on the Play icon to the left of the cell below to importing the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f165089-ff60-415f-8a21-56ac818b20c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openvino\n",
    "%pip install \"ultralytics==8.2.24\"\n",
    "import platform\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "# Inference function\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.solutions import ObjectCounter\n",
    "import cv2\n",
    "import time\n",
    "import collections\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "import torch\n",
    "import openvino as ov\n",
    "#import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f654383d",
   "metadata": {},
   "source": [
    "## Initializing the model\n",
    "\n",
    "Our next step is to initialize the model. Key points about this steps are :\n",
    "- Detection model name is declared. In our case that is \"yolov8n\"\n",
    "- Detection model path is set.\n",
    "- Label map is loaded. The label map tells us what class of objects we can use to accomplish the counting of a particular object. For example, we can use people as class of object, or apples. Label map is the list of all those classes of objects.\n",
    "\n",
    "Click on the Play icon to the left of the cell below to initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbbf1c7-af96-40fd-96e1-04fc648811fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models_dir = Path(\"./models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "DET_MODEL_NAME = \"yolov8n\"\n",
    "\n",
    "det_model = YOLO(models_dir / f\"{DET_MODEL_NAME}.pt\")\n",
    "label_map = det_model.model.names\n",
    "\n",
    "# Need to make en empty call to initialize the model\n",
    "res = det_model()\n",
    "det_model_path = models_dir / f\"{DET_MODEL_NAME}_openvino_model/{DET_MODEL_NAME}.xml\"\n",
    "if not det_model_path.exists():\n",
    "    det_model.export(format=\"openvino\", dynamic=True, half=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936dfbe8",
   "metadata": {},
   "source": [
    "## Setting up of helper functions\n",
    "This excercise is designed to explain each part and having an opportunity to see how each part is defined, the goal of this step is to review and excecute the helper function that make this work. There are two helper functions that we will execute in this step. The first helper function shows how the box around the detected object is defined, what color is used and thickness of the box. The second helper function is about the points. This is used when object is on the move. For example a person.\n",
    "\n",
    "Click on the Play icon to the left of the cell below to setup the helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c1a460c-3d62-464a-ba56-ac218531e975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor=\"green\", facecolor=(0, 0, 0, 0), lw=2))\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels == 1]\n",
    "    neg_points = coords[labels == 0]\n",
    "    ax.scatter(\n",
    "        pos_points[:, 0],\n",
    "        pos_points[:, 1],\n",
    "        color=\"green\",\n",
    "        marker=\"*\",\n",
    "        s=marker_size,\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=1.25,\n",
    "    )\n",
    "    ax.scatter(\n",
    "        neg_points[:, 0],\n",
    "        neg_points[:, 1],\n",
    "        color=\"red\",\n",
    "        marker=\"*\",\n",
    "        s=marker_size,\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=1.25,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c12345",
   "metadata": {},
   "source": [
    "## Setting up the inferencing function\n",
    "The inferencing function is the core of the this excercise. This function takes three parameters. These paramters are:\n",
    "- Source: This parameter tells the inferencing function which video feed to use as the source.\n",
    "- DeviceType: This parameter is related to the device type to use for inferencing. In our example, we are going to use \"CPU\" as the devices type. Other example of this parater is \"GPU\" which we are not using in this exercise.\n",
    "- Object to count - This parameter tells what object to use for counting. Right now we have limited to three objecs, which are \"apple\", \"banana\" and \"person\". If we need more obects, we can definately add more logic to add more objects.\n",
    "\n",
    " Now, let us take a look at what is happening in the function:\n",
    " - Initializing of the OpenVino.\n",
    " - Model is loaded in accordance with the giving \"deviceType\".\n",
    " - Video capture is initialized.\n",
    " - Object counting is initiated.\n",
    " - As each frame is received after object detection, the following annotations are added to output frame:\n",
    "    - Text annoations\n",
    "    - Buonding box\n",
    "    - Points (incase object is in motion)\n",
    "\n",
    "Click on the Play icon to the left of the cell below to setup the inferencing function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755fa832-ac41-4abf-8124-54d2ae4efd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"ultralytics==8.2.24\" opencv-python pyyaml openvino\n",
    "def run_inference(source, deviceType, objectToCount):\n",
    "    core = ov.Core()\n",
    "   \n",
    "    det_ov_model = core.read_model(det_model_path)\n",
    "    ov_config = {}\n",
    "\n",
    "    compiled_model = core.compile_model(det_ov_model, deviceType, ov_config)\n",
    "\n",
    "    def infer(*args):\n",
    "        result = compiled_model(args)\n",
    "        return torch.from_numpy(result[0])\n",
    "\n",
    "    # Use openVINO as inference engine\n",
    "    det_model.predictor.inference = infer\n",
    "    det_model.predictor.model.pt = False\n",
    "\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(source)\n",
    "        assert cap.isOpened(), \"Error reading video file\"\n",
    "        # line_points = [(0, 300), (1080, 300)]  \n",
    "        # line_points = [(0, 1080, 1080, 0, 0), (300, 300,0,0, 300)]  # line or region points\n",
    "        line_points = [(0, 600), (800, 600),(800, 0),(0,0),(0,600)]  # line or region points\n",
    "\n",
    "        classes_to_count = [0]  # person is class 0 in the COCO dataset, 46 is banana and 47 is apples.\n",
    "        if(objectToCount == \"person\"):\n",
    "            classes_to_count == [0]\n",
    "        if(objectToCount == \"banana\"):\n",
    "            classes_to_count = [46]\n",
    "        if(objectToCount == \"apple\"):\n",
    "           classes_to_count = [47]\n",
    "\n",
    "           \n",
    "        # Init Object Counter\n",
    "        counter = ObjectCounter(\n",
    "            view_img=False, reg_pts=line_points, classes_names=det_model.names, draw_tracks=True, line_thickness=1, view_in_counts=False, view_out_counts=False\n",
    "        )\n",
    "        # Processing time\n",
    "        processing_times = collections.deque(maxlen=200)\n",
    "\n",
    "        while cap.isOpened():\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                print(\"Video frame is empty or video processing has been successfully completed.\")\n",
    "                break\n",
    "\n",
    "            start_time = time.time()\n",
    "            tracks = det_model.track(frame, persist=True, show=False, classes=classes_to_count, verbose=False)\n",
    "            frame = counter.start_counting(frame, tracks)\n",
    "            stop_time = time.time()\n",
    "\n",
    "            processing_times.append(stop_time - start_time)\n",
    "\n",
    "            # Mean processing time [ms].\n",
    "            _, f_width = frame.shape[:2]\n",
    "            processing_time = np.mean(processing_times) * 1000\n",
    "            fps = 1000 / processing_time\n",
    "            cv2.putText(\n",
    "                img=frame,\n",
    "                text=f\"Inference time: {processing_time:.1f}ms ({fps:.1f} FPS)\",\n",
    "                org=(20, 40),\n",
    "                fontFace=cv2.FONT_HERSHEY_COMPLEX,\n",
    "                fontScale=f_width / 1000,\n",
    "                color=(0, 0, 255),\n",
    "                thickness=2,\n",
    "                lineType=cv2.LINE_AA,\n",
    "            )\n",
    "\n",
    "            # Get the counts. Counts are getting as 'OUT'\n",
    "            # Modify this logic accordingly\n",
    "            counts = counter.out_counts\n",
    "\n",
    "            # Define the text to display\n",
    "            text = f\"Count: {counts}\"\n",
    "            fontFace = cv2.FONT_HERSHEY_COMPLEX\n",
    "            fontScale = 0.75  # Adjust scale as needed\n",
    "            thickness = 2\n",
    "\n",
    "            # Calculate the size of the text box\n",
    "            (text_width, text_height), _ = cv2.getTextSize(text, fontFace, fontScale, thickness)\n",
    "\n",
    "            # Define the upper right corner for the text\n",
    "            top_right_corner = (frame.shape[1] - text_width - 20, 40)\n",
    "            # Draw the count of \"OUT\" on the frame\n",
    "            cv2.putText(\n",
    "                img=frame,\n",
    "                text=text,\n",
    "                org=(top_right_corner[0], top_right_corner[1]),\n",
    "                fontFace=fontFace,\n",
    "                fontScale=fontScale,\n",
    "                color=(0, 0, 255),\n",
    "                thickness=thickness,\n",
    "                lineType=cv2.LINE_AA,\n",
    "            )\n",
    "\n",
    "            # Show the frame\n",
    "            _, encoded_img = cv2.imencode(ext=\".jpg\", img=frame, params=[cv2.IMWRITE_JPEG_QUALITY, 100])\n",
    "            # Create an IPython image.\n",
    "            i = display.Image(data=encoded_img)\n",
    "            # Display the image in this notebook.          \n",
    "            display.clear_output(wait=True)\n",
    "            display.display(i)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b624fba",
   "metadata": {},
   "source": [
    "## Let us choose what we want to track\n",
    "For this step, assign one of three values to objectToCount variable below. The three options are: person, apple and banana.\n",
    "\n",
    "- To accomplish this, enter the value for \"objectToCount\" variable in the below cell.\n",
    "- Once \"objectToCount\" variable is set, click on the play icon to the left of the cell below to execute assigning of the source vide step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16240b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "objectToCount = \"\"   # \"person\", \"apple\" or \"banana\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6a8c27",
   "metadata": {},
   "source": [
    "## Let us choose the video source.\n",
    "\n",
    "For this step, provide a URL of a Video feed that would be used as the source video for inference.\n",
    "\n",
    "- To accomplish this, enter the value for \"sourceVideo\" variable in the below cell.\n",
    "- Once \"sourceVideo\" variable is set, click on the play icon to the left of the cell below to execute assigning of the source vide step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01d773ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available video 1: https://download.microsoft.com/download/caaf80b6-2394-4fbc-8430-8b41a3206c64/people-are-pushing-carts-along.mp4\n",
    "# Available video 2: https://download.microsoft.com/download/a0ac5d61-60b6-4037-9555-ba5acefeb0c8/people-near-shop-counter-fruit.mp4\n",
    "\n",
    "sourceVideo = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3680b2",
   "metadata": {},
   "source": [
    "## Execution\n",
    "This is the final step of the exercise. In this step, the inferening function that was defined in the previous step, is called and the output result is shown.\n",
    "As mentioned previously, the inferencing function receives the information about the source of the video, the deviceType (in our case it is \"CPU\") and the object to count information from this execution step.\n",
    "\n",
    "Click on the Play icon to the left of the cell below to execute the final step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6455fb8b-384c-4d1e-b227-88e499b66c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "core = ov.Core()\n",
    "deviceType = \"CPU\"\n",
    "\n",
    "if objectToCount == \"\":\n",
    "    print(\"Missing objectToCount\")\n",
    "    print(\"Adding default value\")\n",
    "    objectToCount = \"person\"\n",
    "\n",
    "if sourceVideo == \"\":\n",
    "    print(\"Missing objectToCount\")\n",
    "    print(\"Adding default value\")\n",
    "    sourceVideo = \"https://download.microsoft.com/download/caaf80b6-2394-4fbc-8430-8b41a3206c64/people-are-pushing-carts-along.mp4\"\n",
    "\n",
    "run_inference(\n",
    "    source=sourceVideo,\n",
    "    deviceType=deviceType,\n",
    "    objectToCount=objectToCount\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb75b97e",
   "metadata": {},
   "source": [
    "### Bonus activities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4070b004",
   "metadata": {},
   "source": [
    "#### 1. Adding/removing track lines for person counting\n",
    "\n",
    "In this bonus activity, we will change code to allow track lines to be visible in case we are tracking objects in motion.\n",
    "\n",
    "- To accomplish this, change the \"draw_tracks\" parameter in run_inference function to reflect if tracks are required or not.\n",
    "- Rerun the notebook after making change to test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d60493",
   "metadata": {},
   "source": [
    "#### 2. Change the dimenions of the bounding box\n",
    "\n",
    "In this bonus activity, we will change the code to draw the bounding box with different dimenions.\n",
    "\n",
    "- To accomplish this, change the run_inference function to change the line_points variable.\n",
    "- Example line_points = [(0, 600), (800, 600),(800, 0),(0,0),(0,600)] \n",
    "- Rerun the notebook after making change to test."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
