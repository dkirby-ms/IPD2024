{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (8.3.18)\n",
      "Requirement already satisfied: opencv-python in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: pyyaml in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (6.0.2)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from ultralytics) (2.0.2)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from ultralytics) (3.9.2)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from ultralytics) (11.0.0)\n",
      "Requirement already satisfied: requests>=2.23.0 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from ultralytics) (1.14.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from ultralytics) (2.5.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from ultralytics) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from ultralytics) (4.66.5)\n",
      "Requirement already satisfied: psutil in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from ultralytics) (6.1.0)\n",
      "Requirement already satisfied: py-cpuinfo in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from ultralytics) (2.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
      "Requirement already satisfied: filelock in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ultralytics opencv-python pyyaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision inferencing with a local model\n",
    "\n",
    "In the first exercise, we can test out a basic [computer vision](https://www.microsoft.com/en-us/research/research-area/computer-vision/?msockid=22ee1fda33f46de00ef10b8532d86c89) inferencing task using a popular AI model called [YOLOv8](https://docs.ultralytics.com/models/yolov8/). YOLO (You Only Look Once) is a real-time object detection system that works by processing static images. It divides the image into a grid and predicts bounding boxes and probabilities for each grid cell, allowing it to detect multiple objects within a single image efficiently. \n",
    "\n",
    "To get started we will initialize the model via the Ultralytics python library. This will automatically download the model. Different sizes for the YOLOv8 model can be specified depending on the workload to adjust balance for accuracy versus speed. Once we initialize the model in our code, we can label the detected objects using [COCO dataset](https://cocodataset.org/#overview) class labels. The class labels dataset can be viewed [here](../artifacts/coco.yaml) where you can see the different types of objects that can be potentially identified.\n",
    "\n",
    "Click on the Play icon to the left of the cell below to initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'person',\n",
      " 1: 'bicycle',\n",
      " 2: 'car',\n",
      " 3: 'motorcycle',\n",
      " 4: 'airplane',\n",
      " 5: 'bus',\n",
      " 6: 'train',\n",
      " 7: 'truck',\n",
      " 8: 'boat',\n",
      " 9: 'traffic light',\n",
      " 10: 'fire hydrant',\n",
      " 11: 'stop sign',\n",
      " 12: 'parking meter',\n",
      " 13: 'bench',\n",
      " 14: 'bird',\n",
      " 15: 'cat',\n",
      " 16: 'dog',\n",
      " 17: 'horse',\n",
      " 18: 'sheep',\n",
      " 19: 'cow',\n",
      " 20: 'elephant',\n",
      " 21: 'bear',\n",
      " 22: 'zebra',\n",
      " 23: 'giraffe',\n",
      " 24: 'backpack',\n",
      " 25: 'umbrella',\n",
      " 26: 'handbag',\n",
      " 27: 'tie',\n",
      " 28: 'suitcase',\n",
      " 29: 'frisbee',\n",
      " 30: 'skis',\n",
      " 31: 'snowboard',\n",
      " 32: 'sports ball',\n",
      " 33: 'kite',\n",
      " 34: 'baseball bat',\n",
      " 35: 'baseball glove',\n",
      " 36: 'skateboard',\n",
      " 37: 'surfboard',\n",
      " 38: 'tennis racket',\n",
      " 39: 'bottle',\n",
      " 40: 'wine glass',\n",
      " 41: 'cup',\n",
      " 42: 'fork',\n",
      " 43: 'knife',\n",
      " 44: 'spoon',\n",
      " 45: 'bowl',\n",
      " 46: 'banana',\n",
      " 47: 'apple',\n",
      " 48: 'sandwich',\n",
      " 49: 'orange',\n",
      " 50: 'broccoli',\n",
      " 51: 'carrot',\n",
      " 52: 'hot dog',\n",
      " 53: 'pizza',\n",
      " 54: 'donut',\n",
      " 55: 'cake',\n",
      " 56: 'chair',\n",
      " 57: 'couch',\n",
      " 58: 'potted plant',\n",
      " 59: 'bed',\n",
      " 60: 'dining table',\n",
      " 61: 'toilet',\n",
      " 62: 'tv',\n",
      " 63: 'laptop',\n",
      " 64: 'mouse',\n",
      " 65: 'remote',\n",
      " 66: 'keyboard',\n",
      " 67: 'cell phone',\n",
      " 68: 'microwave',\n",
      " 69: 'oven',\n",
      " 70: 'toaster',\n",
      " 71: 'sink',\n",
      " 72: 'refrigerator',\n",
      " 73: 'book',\n",
      " 74: 'clock',\n",
      " 75: 'vase',\n",
      " 76: 'scissors',\n",
      " 77: 'teddy bear',\n",
      " 78: 'hair drier',\n",
      " 79: 'toothbrush'}\n"
     ]
    }
   ],
   "source": [
    "import cv2, yaml\n",
    "from ultralytics import YOLO\n",
    "from pprint import pprint \n",
    "\n",
    "model = YOLO('yolov8n.pt')  # You can use 'yolov8s.pt', 'yolov8m.pt', etc. for different model sizes\n",
    "\n",
    "# This code loads the class names from the COCO dataset yaml file. \n",
    "def load_class_names(yaml_file):\n",
    "    with open(yaml_file, 'r') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    return data['names']\n",
    "\n",
    "class_names = load_class_names('../artifacts/coco.yaml')  # Adjust the path to your .names file\n",
    "\n",
    "pprint(class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic object detection on a static image\n",
    "\n",
    "The next code block will load an image from disk using the Python [OpenCV](https://opencv.org/) library and send it to the model for basic object detection. Any detected objects will be annotated with a box drawn around them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 10 persons, 2 handbags, 87.2ms\n",
      "Speed: 11.0ms preprocess, 87.2ms inference, 5.4ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    }
   ],
   "source": [
    "# Load image\n",
    "image_path = '../media/image/people_on_street.jpg'\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Perform basic detection\n",
    "results = model(image)\n",
    "\n",
    "# Draw bounding boxes on the image and label objects by referencing the class names\n",
    "for result in results:\n",
    "    for box in result.boxes:\n",
    "        class_id = int(box.cls[0])\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        confidence = box.conf[0]\n",
    "        label = f'{class_names[class_id]} {confidence:.2f}'\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Display the image until with the bounding boxes until a key is pressed\n",
    "cv2.imshow('Press any key to close', image)\n",
    "while True:\n",
    "    if cv2.waitKey(1) != -1:\n",
    "        break\n",
    "    if cv2.getWindowProperty('Press any key to close', cv2.WND_PROP_VISIBLE) < 1:\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object detection in a video file\n",
    "\n",
    "By adjusting our technical implementation, we can detect objects with YOLO inside a video file. \n",
    "\n",
    "To use YOLO with a video file, we need to extract individual frames from the video and then apply the YOLO model to each frame separately. This process involves reading the video file, extracting frames at a specified frame rate, performing object detection on each frame, and then potentially reassembling the processed frames back into a video format. This approach allows us to leverage YOLO's capabilities for real-time object detection in video streams.\n",
    "\n",
    "![A diagram illustrating the video-to-frame concept](./img/video_to_frame_diagram_small.png)\n",
    "\n",
    "Another concept to consider is the rate at which frames are extracted from the video and sent to the model for inferencing. This can be measured in frames-per-second, also known as framerate. At 30 frames per second, we will need to extract 30 individual images from the video stream every second. \n",
    "\n",
    "![A diagram illustrating frames-per-second](./img/fps_diagram.png)\n",
    "\n",
    "Framerate can be adjusted as needed to balance between performance and cost. In our example we will set a framerate of 3, which will result in a moderate amount of frames written to disk for the included video sample file. This in turn will result in less resource cost to run inferencing against our video.\n",
    "\n",
    "Let's use a sample video file and perform this first step to extract frames from a sample video file.\n",
    "\n",
    "Run the next cell using the Play button the left. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frame 1 from ../media/video/sample.mp4.\n",
      "Extracting frame 2 from ../media/video/sample.mp4.\n",
      "Extracting frame 3 from ../media/video/sample.mp4.\n",
      "Extracting frame 4 from ../media/video/sample.mp4.\n",
      "Extracting frame 5 from ../media/video/sample.mp4.\n",
      "Extracting frame 6 from ../media/video/sample.mp4.\n",
      "Extracting frame 7 from ../media/video/sample.mp4.\n",
      "Extracting frame 8 from ../media/video/sample.mp4.\n",
      "Extracting frame 9 from ../media/video/sample.mp4.\n",
      "Extracting frame 10 from ../media/video/sample.mp4.\n",
      "Extracting frame 11 from ../media/video/sample.mp4.\n",
      "Extracting frame 12 from ../media/video/sample.mp4.\n",
      "Extracting frame 13 from ../media/video/sample.mp4.\n",
      "Extracting frame 14 from ../media/video/sample.mp4.\n",
      "Extracting frame 15 from ../media/video/sample.mp4.\n",
      "Extracting frame 16 from ../media/video/sample.mp4.\n",
      "Extracting frame 17 from ../media/video/sample.mp4.\n",
      "Extracting frame 18 from ../media/video/sample.mp4.\n",
      "Extracting frame 19 from ../media/video/sample.mp4.\n",
      "Extracting frame 20 from ../media/video/sample.mp4.\n",
      "Extracting frame 21 from ../media/video/sample.mp4.\n",
      "Extracting frame 22 from ../media/video/sample.mp4.\n",
      "Extracting frame 23 from ../media/video/sample.mp4.\n",
      "Extracting frame 24 from ../media/video/sample.mp4.\n",
      "Extracting frame 25 from ../media/video/sample.mp4.\n",
      "Extracting frame 26 from ../media/video/sample.mp4.\n",
      "Extracting frame 27 from ../media/video/sample.mp4.\n",
      "Extracting frame 28 from ../media/video/sample.mp4.\n",
      "Extracting frame 29 from ../media/video/sample.mp4.\n",
      "Extracting frame 30 from ../media/video/sample.mp4.\n",
      "Extracting frame 31 from ../media/video/sample.mp4.\n",
      "Extracting frame 32 from ../media/video/sample.mp4.\n",
      "Extracting frame 33 from ../media/video/sample.mp4.\n",
      "Extracting frame 34 from ../media/video/sample.mp4.\n",
      "Extracting frame 35 from ../media/video/sample.mp4.\n",
      "Extracting frame 36 from ../media/video/sample.mp4.\n",
      "Extracting frame 37 from ../media/video/sample.mp4.\n",
      "Extracting frame 38 from ../media/video/sample.mp4.\n",
      "Extracting frame 39 from ../media/video/sample.mp4.\n",
      "Extracting frame 40 from ../media/video/sample.mp4.\n",
      "Extracting frame 41 from ../media/video/sample.mp4.\n",
      "Extracting frame 42 from ../media/video/sample.mp4.\n",
      "Extracting frame 43 from ../media/video/sample.mp4.\n",
      "Extracting frame 44 from ../media/video/sample.mp4.\n",
      "Extracting frame 45 from ../media/video/sample.mp4.\n",
      "Extracting frame 46 from ../media/video/sample.mp4.\n",
      "Extracting frame 47 from ../media/video/sample.mp4.\n",
      "Extracting frame 48 from ../media/video/sample.mp4.\n",
      "Extracting frame 49 from ../media/video/sample.mp4.\n",
      "Extracting frame 50 from ../media/video/sample.mp4.\n",
      "Extracting frame 51 from ../media/video/sample.mp4.\n",
      "Extracting frame 52 from ../media/video/sample.mp4.\n",
      "Extracting frame 53 from ../media/video/sample.mp4.\n",
      "Extracting frame 54 from ../media/video/sample.mp4.\n",
      "Extracting frame 55 from ../media/video/sample.mp4.\n",
      "Extracting frame 56 from ../media/video/sample.mp4.\n",
      "Extracting frame 57 from ../media/video/sample.mp4.\n",
      "Extracting frame 58 from ../media/video/sample.mp4.\n",
      "Extracting frame 59 from ../media/video/sample.mp4.\n",
      "Extracting frame 60 from ../media/video/sample.mp4.\n",
      "Extracting frame 61 from ../media/video/sample.mp4.\n",
      "Extracting frame 62 from ../media/video/sample.mp4.\n",
      "Extracting frame 63 from ../media/video/sample.mp4.\n",
      "Extracting frame 64 from ../media/video/sample.mp4.\n",
      "Extracting frame 65 from ../media/video/sample.mp4.\n",
      "Extracting frame 66 from ../media/video/sample.mp4.\n",
      "Extracting frame 67 from ../media/video/sample.mp4.\n",
      "Extracting frame 68 from ../media/video/sample.mp4.\n",
      "Extracting frame 69 from ../media/video/sample.mp4.\n",
      "Extracting frame 70 from ../media/video/sample.mp4.\n",
      "Extracting frame 71 from ../media/video/sample.mp4.\n",
      "Extracting frame 72 from ../media/video/sample.mp4.\n",
      "Extracting frame 73 from ../media/video/sample.mp4.\n",
      "Extracting frame 74 from ../media/video/sample.mp4.\n",
      "Extracting frame 75 from ../media/video/sample.mp4.\n",
      "Extracting frame 76 from ../media/video/sample.mp4.\n",
      "Extracting frame 77 from ../media/video/sample.mp4.\n",
      "Extracting frame 78 from ../media/video/sample.mp4.\n",
      "Extracting frame 79 from ../media/video/sample.mp4.\n",
      "Extracting frame 80 from ../media/video/sample.mp4.\n",
      "Extracting frame 81 from ../media/video/sample.mp4.\n",
      "Extracting frame 82 from ../media/video/sample.mp4.\n",
      "Extracting frame 83 from ../media/video/sample.mp4.\n",
      "Extracting frame 84 from ../media/video/sample.mp4.\n",
      "Extracting frame 85 from ../media/video/sample.mp4.\n",
      "Extracting frame 86 from ../media/video/sample.mp4.\n",
      "Extracting frame 87 from ../media/video/sample.mp4.\n",
      "Extracting frame 88 from ../media/video/sample.mp4.\n",
      "Extracting frame 89 from ../media/video/sample.mp4.\n",
      "Extracting frame 90 from ../media/video/sample.mp4.\n",
      "Extracting frame 91 from ../media/video/sample.mp4.\n",
      "Extracting frame 92 from ../media/video/sample.mp4.\n",
      "Extracting frame 93 from ../media/video/sample.mp4.\n",
      "Extracting frame 94 from ../media/video/sample.mp4.\n",
      "Extracting frame 95 from ../media/video/sample.mp4.\n",
      "Extracting frame 96 from ../media/video/sample.mp4.\n",
      "Extracting frame 97 from ../media/video/sample.mp4.\n",
      "Extracting frame 98 from ../media/video/sample.mp4.\n",
      "Extracting frame 99 from ../media/video/sample.mp4.\n",
      "Extracting frame 100 from ../media/video/sample.mp4.\n",
      "Extracting frame 101 from ../media/video/sample.mp4.\n",
      "Extracting frame 102 from ../media/video/sample.mp4.\n",
      "Extracting frame 103 from ../media/video/sample.mp4.\n",
      "Extracting frame 104 from ../media/video/sample.mp4.\n",
      "Extracting frame 105 from ../media/video/sample.mp4.\n",
      "Extracting frame 106 from ../media/video/sample.mp4.\n",
      "Extracting frame 107 from ../media/video/sample.mp4.\n",
      "Extracting frame 108 from ../media/video/sample.mp4.\n",
      "Extracting frame 109 from ../media/video/sample.mp4.\n",
      "Extracting frame 110 from ../media/video/sample.mp4.\n",
      "Extracting frame 111 from ../media/video/sample.mp4.\n",
      "Extracting frame 112 from ../media/video/sample.mp4.\n",
      "Extracting frame 113 from ../media/video/sample.mp4.\n",
      "Extracting frame 114 from ../media/video/sample.mp4.\n",
      "Extracting frame 115 from ../media/video/sample.mp4.\n",
      "Extracting frame 116 from ../media/video/sample.mp4.\n",
      "Extracting frame 117 from ../media/video/sample.mp4.\n",
      "Extracting frame 118 from ../media/video/sample.mp4.\n",
      "Extracting frame 119 from ../media/video/sample.mp4.\n",
      "Extracting frame 120 from ../media/video/sample.mp4.\n",
      "Extracting frame 121 from ../media/video/sample.mp4.\n",
      "Extracting frame 122 from ../media/video/sample.mp4.\n",
      "Extracting frame 123 from ../media/video/sample.mp4.\n",
      "Extracting frame 124 from ../media/video/sample.mp4.\n",
      "Extracting frame 125 from ../media/video/sample.mp4.\n",
      "Extracting frame 126 from ../media/video/sample.mp4.\n",
      "Extracting frame 127 from ../media/video/sample.mp4.\n",
      "Extracting frame 128 from ../media/video/sample.mp4.\n",
      "Extracting frame 129 from ../media/video/sample.mp4.\n",
      "Extracting frame 130 from ../media/video/sample.mp4.\n",
      "Extracting frame 131 from ../media/video/sample.mp4.\n",
      "Extracting frame 132 from ../media/video/sample.mp4.\n",
      "Extracting frame 133 from ../media/video/sample.mp4.\n",
      "Extracting frame 134 from ../media/video/sample.mp4.\n",
      "Extracting frame 135 from ../media/video/sample.mp4.\n",
      "Extracting frame 136 from ../media/video/sample.mp4.\n",
      "Extracting frame 137 from ../media/video/sample.mp4.\n",
      "Extracting frame 138 from ../media/video/sample.mp4.\n",
      "Extracting frame 139 from ../media/video/sample.mp4.\n",
      "Extracting frame 140 from ../media/video/sample.mp4.\n",
      "Extracting frame 141 from ../media/video/sample.mp4.\n",
      "Extracting frame 142 from ../media/video/sample.mp4.\n",
      "Extracting frame 143 from ../media/video/sample.mp4.\n",
      "Extracting frame 144 from ../media/video/sample.mp4.\n",
      "Extracting frame 145 from ../media/video/sample.mp4.\n",
      "Extracting frame 146 from ../media/video/sample.mp4.\n",
      "Extracting frame 147 from ../media/video/sample.mp4.\n",
      "Extracting frame 148 from ../media/video/sample.mp4.\n",
      "Extracting frame 149 from ../media/video/sample.mp4.\n",
      "Extracting frame 150 from ../media/video/sample.mp4.\n",
      "Extracting frame 151 from ../media/video/sample.mp4.\n",
      "Extracting frame 152 from ../media/video/sample.mp4.\n",
      "Extracting frame 153 from ../media/video/sample.mp4.\n",
      "Extracting frame 154 from ../media/video/sample.mp4.\n",
      "Extracting frame 155 from ../media/video/sample.mp4.\n",
      "Extracting frame 156 from ../media/video/sample.mp4.\n",
      "Extracting frame 157 from ../media/video/sample.mp4.\n",
      "Extracting frame 158 from ../media/video/sample.mp4.\n",
      "Extracting frame 159 from ../media/video/sample.mp4.\n",
      "Extracting frame 160 from ../media/video/sample.mp4.\n",
      "Extracting frame 161 from ../media/video/sample.mp4.\n",
      "Extracting frame 162 from ../media/video/sample.mp4.\n",
      "Extracting frame 163 from ../media/video/sample.mp4.\n",
      "Extracting frame 164 from ../media/video/sample.mp4.\n",
      "Extracting frame 165 from ../media/video/sample.mp4.\n",
      "Extracting frame 166 from ../media/video/sample.mp4.\n",
      "Extracting frame 167 from ../media/video/sample.mp4.\n",
      "Extracting frame 168 from ../media/video/sample.mp4.\n",
      "Extracting frame 169 from ../media/video/sample.mp4.\n",
      "Extracting frame 170 from ../media/video/sample.mp4.\n",
      "Extracting frame 171 from ../media/video/sample.mp4.\n",
      "Extracting frame 172 from ../media/video/sample.mp4.\n",
      "Extracting frame 173 from ../media/video/sample.mp4.\n",
      "Extracting frame 174 from ../media/video/sample.mp4.\n",
      "Extracting frame 175 from ../media/video/sample.mp4.\n",
      "Extracting frame 176 from ../media/video/sample.mp4.\n",
      "Extracting frame 177 from ../media/video/sample.mp4.\n",
      "Extracting frame 178 from ../media/video/sample.mp4.\n",
      "Extracting frame 179 from ../media/video/sample.mp4.\n",
      "Extracting frame 180 from ../media/video/sample.mp4.\n",
      "Extracting frame 181 from ../media/video/sample.mp4.\n",
      "Extracting frame 182 from ../media/video/sample.mp4.\n",
      "Extracting frame 183 from ../media/video/sample.mp4.\n",
      "Extracting frame 184 from ../media/video/sample.mp4.\n",
      "Extracting frame 185 from ../media/video/sample.mp4.\n",
      "Extracting frame 186 from ../media/video/sample.mp4.\n",
      "Extracting frame 187 from ../media/video/sample.mp4.\n",
      "Extracting frame 188 from ../media/video/sample.mp4.\n",
      "Extracting frame 189 from ../media/video/sample.mp4.\n",
      "Extracting frame 190 from ../media/video/sample.mp4.\n",
      "Extracting frame 191 from ../media/video/sample.mp4.\n",
      "Extracting frame 192 from ../media/video/sample.mp4.\n",
      "Extracting frame 193 from ../media/video/sample.mp4.\n",
      "Extracting frame 194 from ../media/video/sample.mp4.\n",
      "Extracting frame 195 from ../media/video/sample.mp4.\n",
      "Extracting frame 196 from ../media/video/sample.mp4.\n",
      "Extracting frame 197 from ../media/video/sample.mp4.\n",
      "Extracting frame 198 from ../media/video/sample.mp4.\n",
      "Extracting frame 199 from ../media/video/sample.mp4.\n",
      "Extracting frame 200 from ../media/video/sample.mp4.\n",
      "Extracting frame 201 from ../media/video/sample.mp4.\n",
      "Extracting frame 202 from ../media/video/sample.mp4.\n",
      "Extracting frame 203 from ../media/video/sample.mp4.\n",
      "Extracting frame 204 from ../media/video/sample.mp4.\n",
      "Extracting frame 205 from ../media/video/sample.mp4.\n",
      "Extracting frame 206 from ../media/video/sample.mp4.\n",
      "Extracting frame 207 from ../media/video/sample.mp4.\n",
      "Extracting frame 208 from ../media/video/sample.mp4.\n",
      "Extracting frame 209 from ../media/video/sample.mp4.\n",
      "Extracting frame 210 from ../media/video/sample.mp4.\n",
      "Extracting frame 211 from ../media/video/sample.mp4.\n",
      "Extracting frame 212 from ../media/video/sample.mp4.\n",
      "Extracting frame 213 from ../media/video/sample.mp4.\n",
      "Extracting frame 214 from ../media/video/sample.mp4.\n",
      "Extracting frame 215 from ../media/video/sample.mp4.\n",
      "Extracting frame 216 from ../media/video/sample.mp4.\n",
      "Extracting frame 217 from ../media/video/sample.mp4.\n",
      "Extracting frame 218 from ../media/video/sample.mp4.\n",
      "Extracting frame 219 from ../media/video/sample.mp4.\n",
      "Extracting frame 220 from ../media/video/sample.mp4.\n",
      "Extracting frame 221 from ../media/video/sample.mp4.\n",
      "Extracting frame 222 from ../media/video/sample.mp4.\n",
      "Extracting frame 223 from ../media/video/sample.mp4.\n",
      "Extracting frame 224 from ../media/video/sample.mp4.\n",
      "Extracting frame 225 from ../media/video/sample.mp4.\n",
      "Extracting frame 226 from ../media/video/sample.mp4.\n",
      "Extracting frame 227 from ../media/video/sample.mp4.\n",
      "Extracting frame 228 from ../media/video/sample.mp4.\n",
      "Extracting frame 229 from ../media/video/sample.mp4.\n",
      "Extracting frame 230 from ../media/video/sample.mp4.\n",
      "Extracting frame 231 from ../media/video/sample.mp4.\n",
      "Extracting frame 232 from ../media/video/sample.mp4.\n",
      "Extracting frame 233 from ../media/video/sample.mp4.\n",
      "Extracting frame 234 from ../media/video/sample.mp4.\n",
      "Extracting frame 235 from ../media/video/sample.mp4.\n",
      "Extracting frame 236 from ../media/video/sample.mp4.\n",
      "Extracting frame 237 from ../media/video/sample.mp4.\n",
      "Extracting frame 238 from ../media/video/sample.mp4.\n",
      "Extracting frame 239 from ../media/video/sample.mp4.\n",
      "Extracting frame 240 from ../media/video/sample.mp4.\n",
      "Extracting frame 241 from ../media/video/sample.mp4.\n",
      "Extracting frame 242 from ../media/video/sample.mp4.\n",
      "Extracting frame 243 from ../media/video/sample.mp4.\n",
      "Extracting frame 244 from ../media/video/sample.mp4.\n",
      "Extracting frame 245 from ../media/video/sample.mp4.\n",
      "Extracting frame 246 from ../media/video/sample.mp4.\n",
      "Extracting frame 247 from ../media/video/sample.mp4.\n",
      "Extracting frame 248 from ../media/video/sample.mp4.\n",
      "Extracting frame 249 from ../media/video/sample.mp4.\n",
      "Extracting frame 250 from ../media/video/sample.mp4.\n",
      "Extracting frame 251 from ../media/video/sample.mp4.\n",
      "Extracting frame 252 from ../media/video/sample.mp4.\n",
      "Extracting frame 253 from ../media/video/sample.mp4.\n",
      "Extracting frame 254 from ../media/video/sample.mp4.\n",
      "Extracted 32 frames from the video.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "video_path = '../media/video/sample.mp4'\n",
    "#video_path = '../media/video/sample2.mp4'\n",
    "video_filename = os.path.splitext(os.path.basename(video_path))[0]\n",
    "output_folder='../video_frames/' + video_filename\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "frame_rate = 3 # Set the frame rate (frames per second)\n",
    "cap = cv2.VideoCapture(video_path) # Open the video file\n",
    "\n",
    "# Get the total number of frames in the video and calculate the interval between frames to capture\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) \n",
    "frame_interval = int(cap.get(cv2.CAP_PROP_FPS) / frame_rate)\n",
    "frame_count = 0\n",
    "saved_frame_count = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Save the frame if it is at the specified interval\n",
    "    if frame_count % frame_interval == 0:\n",
    "        frame_filename = os.path.join(output_folder, f'frame_{saved_frame_count:04d}.jpg')\n",
    "        cv2.imwrite(frame_filename, frame)\n",
    "        saved_frame_count += 1\n",
    "\n",
    "    frame_count += 1\n",
    "    print(f\"Extracting frame {frame_count} from {video_path}.\")\n",
    "    \n",
    "cap.release()\n",
    "print(f\"Extracted {saved_frame_count} frames from the video.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 6 persons, 45.3ms\n",
      "Speed: 6.8ms preprocess, 45.3ms inference, 10.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1 boat, 12.5ms\n",
      "Speed: 1.8ms preprocess, 12.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 1 boat, 6.9ms\n",
      "Speed: 1.4ms preprocess, 6.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 9.5ms\n",
      "Speed: 1.7ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 5.4ms\n",
      "Speed: 1.6ms preprocess, 5.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 1 apple, 6.4ms\n",
      "Speed: 1.4ms preprocess, 6.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 2 apples, 5.3ms\n",
      "Speed: 2.3ms preprocess, 5.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 2 apples, 8.2ms\n",
      "Speed: 1.5ms preprocess, 8.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 16.6ms\n",
      "Speed: 2.7ms preprocess, 16.6ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 2 apples, 14.3ms\n",
      "Speed: 1.9ms preprocess, 14.3ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 apple, 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 apple, 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 1 apple, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 17.9ms\n",
      "Speed: 2.9ms preprocess, 17.9ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 1 motorcycle, 15.2ms\n",
      "Speed: 1.7ms preprocess, 15.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 2 backpacks, 1 apple, 5.6ms\n",
      "Speed: 1.6ms preprocess, 5.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 2 backpacks, 2 apples, 12.9ms\n",
      "Speed: 1.4ms preprocess, 12.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 2 backpacks, 1 apple, 8.2ms\n",
      "Speed: 1.3ms preprocess, 8.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 1 motorcycle, 3 backpacks, 11.9ms\n",
      "Speed: 1.5ms preprocess, 11.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 1 truck, 1 boat, 1 backpack, 10.6ms\n",
      "Speed: 1.5ms preprocess, 10.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 1 boat, 1 backpack, 1 skateboard, 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 1 truck, 1 backpack, 2 skateboards, 6.1ms\n",
      "Speed: 1.8ms preprocess, 6.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 1 truck, 1 backpack, 1 skateboard, 15.0ms\n",
      "Speed: 2.6ms preprocess, 15.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 2 backpacks, 9.6ms\n",
      "Speed: 1.4ms preprocess, 9.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 2 backpacks, 1 snowboard, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 2 backpacks, 1 snowboard, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 2 backpacks, 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 2 backpacks, 6.0ms\n",
      "Speed: 1.5ms preprocess, 6.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 2 backpacks, 6.0ms\n",
      "Speed: 1.4ms preprocess, 6.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 2 backpacks, 6.1ms\n",
      "Speed: 1.6ms preprocess, 6.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 2 backpacks, 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 backpack, 13.6ms\n",
      "Speed: 1.6ms preprocess, 13.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 1 truck, 2 backpacks, 11.1ms\n",
      "Speed: 2.7ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 truck, 1 backpack, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 backpack, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 1 truck, 1 backpack, 7.7ms\n",
      "Speed: 1.4ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 truck, 2 backpacks, 12.9ms\n",
      "Speed: 2.0ms preprocess, 12.9ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 truck, 1 backpack, 7.8ms\n",
      "Speed: 1.8ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 truck, 2 backpacks, 7.9ms\n",
      "Speed: 1.4ms preprocess, 7.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 1 truck, 2 backpacks, 1 skateboard, 9.1ms\n",
      "Speed: 1.7ms preprocess, 9.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 truck, 2 backpacks, 1 skateboard, 9.3ms\n",
      "Speed: 1.4ms preprocess, 9.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 backpack, 7.9ms\n",
      "Speed: 1.5ms preprocess, 7.9ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "# Load the YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Load video\n",
    "video_path = '../media/video/sample.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "delay = 1\n",
    "\n",
    "# Get video writer initialized to save the output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output_video.avi', fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Perform detection\n",
    "    results = model(frame)\n",
    "\n",
    "    # Draw bounding boxes on the frame\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            class_id = int(box.cls[0])\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            confidence = box.conf[0]\n",
    "            label = f'{class_names[class_id]} {confidence:.2f}'\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "    # Write the frame into the output video\n",
    "    out.write(frame)\n",
    "\n",
    "    # Display the frame until q is pressed\n",
    "    cv2.imshow('Detected People (press Q to exit)', frame)\n",
    "    if cv2.waitKey(delay) & 0xFF == ord('q'): \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release() \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision inferencing with Azure OpenAI and GPT4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "Failed to make the request. Error: 401 Client Error: PermissionDenied for url: https://jsextoai.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-08-01-preview",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Failed to make the request. Error: 401 Client Error: PermissionDenied for url: https://jsextoai.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-08-01-preview\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dakir/IgnitePreDay/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import base64\n",
    "from pprint import pprint\n",
    "\n",
    "IMAGE_PATH=\"../media/image/people_on_street.jpg\"\n",
    "ENDPOINT = \"https://jsextoai.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-08-01-preview\"\n",
    "API_KEY = \"xxxxxx\"\n",
    "\n",
    "encoded_image = base64.b64encode(open(IMAGE_PATH, 'rb').read()).decode('ascii')\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": API_KEY,\n",
    "}\n",
    "\n",
    "# Payload for the request\n",
    "payload = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"You are an AI assistant that helps people find information.\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"temperature\": 0.7,\n",
    "  \"top_p\": 0.95,\n",
    "  \"max_tokens\": 800\n",
    "}\n",
    "\n",
    "\n",
    "# Send request\n",
    "try:\n",
    "    response = requests.post(ENDPOINT, headers=headers, json=payload)\n",
    "    response.raise_for_status()  # Will raise an HTTPError if the HTTP request returned an unsuccessful status code\n",
    "except requests.RequestException as e:\n",
    "    raise SystemExit(f\"Failed to make the request. Error: {e}\")\n",
    "\n",
    "# Handle the response as needed (e.g., print or process)\n",
    "response_json = response.json()\n",
    "pprint(response_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "def image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "\n",
    "IMAGE_PATH = \"./images/columns.png\"\n",
    "image_base64 = image_to_base64(IMAGE_PATH)\n",
    "payload = {\n",
    "  \"messages\": [\n",
    "    { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "        { \"role\": \"user\", \"content\": [  \n",
    "            { \n",
    "                \"type\": \"text\", \n",
    "                \"text\": \"Describe this picture:\" \n",
    "            },\n",
    "            { \n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": \"data:image/png;base64,\" + image_base64\n",
    "                }\n",
    "            }\n",
    "        ] } \n",
    "  ],\n",
    "  \"temperature\": 0.7,\n",
    "  \"top_p\": 0.95,\n",
    "  \"max_tokens\": 800\n",
    "}\n",
    "\n",
    "response = requests.post(ENDPOINT, headers=headers, json=payload)\n",
    "response_json = response.json()\n",
    "\n",
    "# Pretty print the JSON response\n",
    "pprint(response_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
