{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ultralytics opencv-python pyyaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision inferencing with a local model\n",
    "\n",
    "In the first exercise, we can test out a basic [computer vision](https://www.microsoft.com/en-us/research/research-area/computer-vision/?msockid=22ee1fda33f46de00ef10b8532d86c89) inferencing task using a popular AI model called [YOLOv8](https://docs.ultralytics.com/models/yolov8/). YOLO (You Only Look Once) is a real-time object detection system that works by processing static images. It divides the image into a grid and predicts bounding boxes and probabilities for each grid cell, allowing it to detect multiple objects within a single image efficiently. \n",
    "\n",
    "To get started we will initialize the model via the Ultralytics python library. This will automatically download the model. Different sizes for the YOLOv8 model can be specified depending on the workload to adjust balance for accuracy versus speed. Once we initialize the model in our code, we can label the detected objects using [COCO dataset](https://cocodataset.org/#overview) class labels. The class labels dataset can be viewed [here](../artifacts/coco.yaml) where you can see the different types of objects that can be potentially identified.\n",
    "\n",
    "Click on the Play icon to the left of the cell below to initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, yaml\n",
    "from ultralytics import YOLO\n",
    "from pprint import pprint \n",
    "\n",
    "model = YOLO('yolov8n.pt')  # You can use 'yolov8s.pt', 'yolov8m.pt', etc. for different model sizes\n",
    "\n",
    "# This code loads the class names from the COCO dataset yaml file. \n",
    "def load_class_names(yaml_file):\n",
    "    with open(yaml_file, 'r') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    return data['names']\n",
    "\n",
    "class_names = load_class_names('../artifacts/coco.yaml')  # Adjust the path to your .names file\n",
    "\n",
    "pprint(class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic object detection on a static image\n",
    "\n",
    "The next code block will load an image from disk using the Python [OpenCV](https://opencv.org/) library and send it to the model for basic object detection. Any detected objects will be annotated with a box drawn around them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "image_path = '../media/image/people_on_street.jpg'\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Perform basic detection\n",
    "results = model(image)\n",
    "\n",
    "# Draw bounding boxes on the image and label objects by referencing the class names\n",
    "for result in results:\n",
    "    for box in result.boxes:\n",
    "        class_id = int(box.cls[0])\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        confidence = box.conf[0]\n",
    "        label = f'{class_names[class_id]} {confidence:.2f}'\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Display the image until with the bounding boxes until a key is pressed\n",
    "cv2.imshow('Press any key to close', image)\n",
    "while True:\n",
    "    if cv2.waitKey(1) != -1:\n",
    "        break\n",
    "    if cv2.getWindowProperty('Press any key to close', cv2.WND_PROP_VISIBLE) < 1:\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object detection in a video file\n",
    "\n",
    "By adjusting our technical implementation, we can detect objects with YOLO inside a video file. \n",
    "\n",
    "To use YOLO with a video file, we need to extract individual frames from the video and then apply the YOLO model to each frame separately. This process involves reading the video file, extracting frames at a specified frame rate, performing object detection on each frame, and then potentially reassembling the processed frames back into a video format. This approach allows us to leverage YOLO's capabilities for real-time object detection in video streams.\n",
    "\n",
    "![A diagram illustrating the video-to-frame concept](./img/video_to_frame_diagram_small.png)\n",
    "\n",
    "Another concept to consider is the rate at which frames are extracted from the video and sent to the model for inferencing. This can be measured in frames-per-second, also known as framerate. At 30 frames per second, we will need to extract 30 individual images from the video stream every second. \n",
    "\n",
    "![A diagram illustrating frames-per-second](./img/fps_diagram.png)\n",
    "\n",
    "Framerate can be adjusted as needed to balance between performance and cost. In our example we will set a framerate of 3, which will result in a moderate amount of frames written to disk for the included video sample file. This in turn will result in less resource cost to run inferencing against our video.\n",
    "\n",
    "Let's use a sample video file and perform this first step to extract frames from a sample video file.\n",
    "\n",
    "Run the next cell using the Play button the left. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "video_path = '../media/video/sample.mp4'\n",
    "#video_path = '../media/video/sample2.mp4'\n",
    "video_filename = os.path.splitext(os.path.basename(video_path))[0]\n",
    "output_folder='../video_frames/' + video_filename\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "frame_skip = 3 # Set the frame skip rate\n",
    "cap = cv2.VideoCapture(video_path) # Open the video file\n",
    "\n",
    "# Get the total number of frames in the video and calculate the interval between frames to capture\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) \n",
    "frame_interval = int(cap.get(cv2.CAP_PROP_FPS) / frame_skip)\n",
    "frame_count = 0\n",
    "saved_frame_count = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Save the frame if it is at the specified interval\n",
    "    if frame_count % frame_interval == 0:\n",
    "        frame_filename = os.path.join(output_folder, f'frame_{saved_frame_count:04d}.jpg')\n",
    "        cv2.imwrite(frame_filename, frame)\n",
    "        saved_frame_count += 1\n",
    "\n",
    "    frame_count += 1\n",
    "    print(f\"Extracting frame {frame_count} from {video_path}.\")\n",
    "    \n",
    "cap.release()\n",
    "print(f\"Extracted {saved_frame_count} frames from the video.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Load video\n",
    "video_path = '../media/video/sample.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "delay = 1\n",
    "\n",
    "# Get video writer initialized to save the output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output_video.avi', fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Perform detection\n",
    "    results = model(frame)\n",
    "\n",
    "    # Draw bounding boxes on the frame\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            class_id = int(box.cls[0])\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            confidence = box.conf[0]\n",
    "            label = f'{class_names[class_id]} {confidence:.2f}'\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "    # Write the frame into the output video\n",
    "    out.write(frame)\n",
    "\n",
    "    # Display the frame until q is pressed\n",
    "    cv2.imshow('Detected People (press Q to exit)', frame)\n",
    "    if cv2.waitKey(delay) & 0xFF == ord('q'): \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release() \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision inferencing with Azure OpenAI and GPT4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import base64\n",
    "from pprint import pprint\n",
    "\n",
    "IMAGE_PATH=\"../media/image/people_on_street.jpg\"\n",
    "ENDPOINT = \"https://jsextoai.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-08-01-preview\"\n",
    "API_KEY = \"xxxxxx\"\n",
    "\n",
    "encoded_image = base64.b64encode(open(IMAGE_PATH, 'rb').read()).decode('ascii')\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": API_KEY,\n",
    "}\n",
    "\n",
    "# Payload for the request\n",
    "payload = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"You are an AI assistant that helps people find information.\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"temperature\": 0.7,\n",
    "  \"top_p\": 0.95,\n",
    "  \"max_tokens\": 800\n",
    "}\n",
    "\n",
    "\n",
    "# Send request\n",
    "try:\n",
    "    response = requests.post(ENDPOINT, headers=headers, json=payload)\n",
    "    response.raise_for_status()  # Will raise an HTTPError if the HTTP request returned an unsuccessful status code\n",
    "except requests.RequestException as e:\n",
    "    raise SystemExit(f\"Failed to make the request. Error: {e}\")\n",
    "\n",
    "# Handle the response as needed (e.g., print or process)\n",
    "response_json = response.json()\n",
    "pprint(response_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "def image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "\n",
    "IMAGE_PATH = \"./images/columns.png\"\n",
    "image_base64 = image_to_base64(IMAGE_PATH)\n",
    "payload = {\n",
    "  \"messages\": [\n",
    "    { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "        { \"role\": \"user\", \"content\": [  \n",
    "            { \n",
    "                \"type\": \"text\", \n",
    "                \"text\": \"Describe this picture:\" \n",
    "            },\n",
    "            { \n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": \"data:image/png;base64,\" + image_base64\n",
    "                }\n",
    "            }\n",
    "        ] } \n",
    "  ],\n",
    "  \"temperature\": 0.7,\n",
    "  \"top_p\": 0.95,\n",
    "  \"max_tokens\": 800\n",
    "}\n",
    "\n",
    "response = requests.post(ENDPOINT, headers=headers, json=payload)\n",
    "response_json = response.json()\n",
    "\n",
    "# Pretty print the JSON response\n",
    "pprint(response_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
