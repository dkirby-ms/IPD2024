{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ultralytics opencv-python pyyaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision inferencing with a local model\n",
    "\n",
    "In the first exercise, we can test out a basic [computer vision](https://www.microsoft.com/en-us/research/research-area/computer-vision/?msockid=22ee1fda33f46de00ef10b8532d86c89) inferencing task using a popular AI model called [YOLOv8](https://docs.ultralytics.com/models/yolov8/). YOLO (You Only Look Once) is a real-time object detection system that works by processing static images. It divides the image into a grid and predicts bounding boxes and probabilities for each grid cell, allowing it to detect multiple objects within a single image efficiently. \n",
    "\n",
    "To get started we will initialize the model via the Ultralytics python library. This will automatically download the model. Different sizes for the YOLOv8 model can be specified depending on the workload to adjust balance for accuracy versus speed. Once we initialize the model in our code, we can label the detected objects using [COCO dataset](https://cocodataset.org/#overview) class labels. The class labels dataset can be viewed [here](../artifacts/coco.yaml) where you can see the different types of objects that can be potentially identified.\n",
    "\n",
    "Click on the Play icon to the left of the cell below to initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, yaml\n",
    "from ultralytics import YOLO\n",
    "from pprint import pprint \n",
    "\n",
    "model = YOLO('yolov8n.pt')  # You can use 'yolov8s.pt', 'yolov8m.pt', etc. for different model sizes\n",
    "\n",
    "# This code loads the class names from the COCO dataset yaml file. \n",
    "def load_class_names(yaml_file):\n",
    "    with open(yaml_file, 'rb') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    return data['names']\n",
    "\n",
    "class_names = load_class_names('../artifacts/coco.yaml')  # Adjust the path to your .names file\n",
    "\n",
    "pprint(class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic object detection on a static image\n",
    "\n",
    "The next code block will load an image from disk using the Python [OpenCV](https://opencv.org/) library and send it to the model for basic object detection. Any detected objects will be annotated with a box drawn around them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "image_path = '../media/image/people_on_street.jpg'\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Perform basic detection\n",
    "results = model(image)\n",
    "\n",
    "# Draw bounding boxes on the image and label objects by referencing the class names\n",
    "for result in results:\n",
    "    for box in result.boxes:\n",
    "        class_id = int(box.cls[0])\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        confidence = box.conf[0]\n",
    "        label = f'{class_names[class_id]} {confidence:.2f}'\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Display the image until with the bounding boxes until a key is pressed\n",
    "cv2.imshow('Press any key to close', image)\n",
    "while True:\n",
    "    if cv2.waitKey(1) != -1:\n",
    "        break\n",
    "    if cv2.getWindowProperty('Press any key to close', cv2.WND_PROP_VISIBLE) < 1:\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object detection in a video file\n",
    "\n",
    "By adjusting our technical implementation, we can detect objects with YOLO inside a video file. \n",
    "\n",
    "To use YOLO with a video file, we need to extract individual frames from the video and then apply the YOLO model to each frame separately. This process involves reading the video file, extracting frames at a specified frame rate, performing object detection on each frame, and then potentially reassembling the processed frames back into a video format. This approach allows us to leverage YOLO's capabilities for real-time object detection in video streams.\n",
    "\n",
    "![A diagram illustrating the video-to-frame concept](./img/video_to_frame_diagram_small.png)\n",
    "\n",
    "Another concept to consider is the rate at which frames are extracted from the video and sent to the model for inferencing. This can be measured in frames-per-second, also known as framerate. At 30 frames per second, we will need to extract 30 individual images from the video stream every second. \n",
    "\n",
    "![A diagram illustrating frames-per-second](./img/fps_diagram.png)\n",
    "\n",
    "Framerate can be adjusted as needed to balance between performance and cost. In our example we will set a framerate of 3, which will result in a moderate amount of frames written to disk for the included video sample file. This in turn will result in less resource cost to run inferencing against our video.\n",
    "\n",
    "Let's use a sample video file and perform this first step to extract frames from a sample video file.\n",
    "\n",
    "Run the next cell using the Play button the left. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "video_path = '../media/video/sample.mp4'\n",
    "#video_path = '../media/video/sample2.mp4'\n",
    "video_filename = os.path.splitext(os.path.basename(video_path))[0]\n",
    "output_folder='../video_frames/' + video_filename\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "frame_skip = 3 # Set the frame skip rate\n",
    "cap = cv2.VideoCapture(video_path) # Open the video file\n",
    "\n",
    "# Get the total number of frames in the video and calculate the interval between frames to capture\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) \n",
    "frame_interval = int(cap.get(cv2.CAP_PROP_FPS) / frame_skip)\n",
    "frame_count = 0\n",
    "saved_frame_count = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Save the frame if it is at the specified interval\n",
    "    if frame_count % frame_interval == 0:\n",
    "        frame_filename = os.path.join(output_folder, f'frame_{saved_frame_count:04d}.jpg')\n",
    "        cv2.imwrite(frame_filename, frame)\n",
    "        saved_frame_count += 1\n",
    "\n",
    "    frame_count += 1\n",
    "    print(f\"Extracting frame {frame_count} from {video_path}.\")\n",
    "    \n",
    "cap.release()\n",
    "print(f\"Extracted {saved_frame_count} frames from the video.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Load video\n",
    "video_path = '../media/video/sample.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "delay = 1\n",
    "\n",
    "# Get video writer initialized to save the output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output_video.avi', fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Perform detection\n",
    "    results = model(frame)\n",
    "\n",
    "    # Draw bounding boxes on the frame\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            class_id = int(box.cls[0])\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            confidence = box.conf[0]\n",
    "            label = f'{class_names[class_id]} {confidence:.2f}'\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "    # Write the frame into the output video\n",
    "    out.write(frame)\n",
    "\n",
    "    # Display the frame until q is pressed\n",
    "    cv2.imshow('Detected People (press Q to exit)', frame)\n",
    "    if cv2.waitKey(delay) & 0xFF == ord('q'): \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release() \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Other object detection models**\n",
    "\n",
    "YOLOv8 (You Only Look Once version 8) is a popular computer vision model known for its speed and accuracy in real-time object detection. It is designed to detect multiple objects within an image or video frame in a single pass, making it highly efficient for applications requiring quick and precise object identification. However, YOLOv8 is just one of many object detection models available. Other notable models include [Faster R-CNN](https://arxiv.org/abs/1506.01497), which provides high accuracy by using region proposal networks, and [SSD (Single Shot MultiBox Detector)](https://arxiv.org/abs/1512.02325), which balances speed and accuracy by detecting objects in a single shot without requiring a region proposal stage.\n",
    "\n",
    "### **Other Vision Inferencing Tasks**\n",
    "\n",
    "Beyond object detection, computer vision encompasses various other inferencing tasks such as image classification, semantic segmentation, and instance segmentation. Image classification involves categorizing an entire image into a predefined class, using models like [ResNet](https://arxiv.org/abs/1512.03385) and [Inception](https://arxiv.org/abs/1512.00567). Semantic segmentation assigns a class label to each pixel in an image, enabling detailed scene understanding, with models like [U-Net](https://arxiv.org/abs/1505.04597) and [DeepLab](https://arxiv.org/abs/1606.00915) excelling in this area. Instance segmentation combines object detection and semantic segmentation to identify and segment each object instance within an image, with models like [Mask R-CNN](https://arxiv.org/abs/1703.06870) being widely used for this purpose. These diverse inferencing tasks enable a broad range of applications, from medical imaging to autonomous driving.\n",
    "\n",
    "For more information, you can explore the following resources:\n",
    "- [YOLO: You Only Look Once](https://pjreddie.com/darknet/yolo/)\n",
    "- [Faster R-CNN](https://arxiv.org/abs/1506.01497)\n",
    "- [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325)\n",
    "- [ResNet: Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n",
    "- [Inception: Going Deeper with Convolutions](https://arxiv.org/abs/1512.00567)\n",
    "- [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)\n",
    "- [DeepLab: Semantic Image Segmentation with Deep Convolutional Nets](https://arxiv.org/abs/1606.00915)\n",
    "- [Mask R-CNN](https://arxiv.org/abs/1703.06870)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Azure AI Services**\n",
    "\n",
    "Login to Azure using the Azure CLI and retrieve your service endpoint and key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP\n",
    "!az login\n",
    "!az cognitiveservices account list --resource-group \"rg-Edge\" -o table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send a prompt to initialize the model\n",
    "\n",
    "Replace the placeholders in the code block below endpoint and key you retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import base64\n",
    "from pprint import pprint\n",
    "\n",
    "IMAGE_PATH=\"../media/image/people_on_street.jpg\"\n",
    "ENDPOINT = \"https://eastus2.api.cognitive.microsoft.com/openai/deployments/openai61ca1-gpt-4o-deployment/chat/completions?api-version=2024-08-01-preview\"\n",
    "API_KEY = \"XXXX\"\n",
    "\n",
    "encoded_image = base64.b64encode(open(IMAGE_PATH, 'rb').read()).decode('ascii')\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": API_KEY,\n",
    "}\n",
    "\n",
    "# Payload for the request\n",
    "payload = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"You are an AI assistant that helps people find information.\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"temperature\": 0.7,\n",
    "  \"top_p\": 0.95,\n",
    "  \"max_tokens\": 800\n",
    "}\n",
    "\n",
    "\n",
    "# Send request\n",
    "try:\n",
    "    response = requests.post(ENDPOINT, headers=headers, json=payload)\n",
    "    response.raise_for_status()  # Will raise an HTTPError if the HTTP request returned an unsuccessful status code\n",
    "except requests.RequestException as e:\n",
    "    raise SystemExit(f\"Failed to make the request. Error: {e}\")\n",
    "\n",
    "# Handle the response as needed (e.g., print or process)\n",
    "response_json = response.json()\n",
    "pprint(response_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe an image with a prompt\n",
    "\n",
    "To send an image with our prompt we will convert it to base64 and include it in the payload of our API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "def image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "\n",
    "IMAGE_PATH = \"../media/image/columns.png\"\n",
    "image_base64 = image_to_base64(IMAGE_PATH)\n",
    "payload = {\n",
    "  \"messages\": [\n",
    "    { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "        { \"role\": \"user\", \"content\": [  \n",
    "            { \n",
    "                \"type\": \"text\", \n",
    "                \"text\": \"Describe this picture:\" \n",
    "            },\n",
    "            { \n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": \"data:image/png;base64,\" + image_base64\n",
    "                }\n",
    "            }\n",
    "        ] } \n",
    "  ],\n",
    "  \"temperature\": 0.7,\n",
    "  \"top_p\": 0.95,\n",
    "  \"max_tokens\": 800\n",
    "}\n",
    "\n",
    "response = requests.post(ENDPOINT, headers=headers, json=payload)\n",
    "response_json = response.json()\n",
    "\n",
    "# Pretty print the JSON response\n",
    "pprint(response_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Lab\n",
    "Run the following cell to complete this Lab!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Task already completed by user'}\n"
     ]
    }
   ],
   "source": [
    "%store -r userId\n",
    "import requests;print(requests.post(\"https://jsleaderboard001-cnece0effvapgbft.westus2-01.azurewebsites.net/complete_task\", headers={\"Content-Type\": \"application/json\"}, json={\"user_id\": userId, \"task_id\": 2}).json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
